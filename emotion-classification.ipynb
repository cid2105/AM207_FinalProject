{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import lda\n",
    "import sklearn\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMOTION_MAP = {0:'joy', 1:'anger', 2:'disgust', 3:'fear', 4:'guilt', 5:'sadness', 6:'shame'}\n",
    "docs = open('corpus.txt').readlines()\n",
    "labels = np.array(np.loadtxt('classes.txt'), dtype=np.int32)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = doc.lower()\n",
    "    no_punctuation = doc.translate(None, string.punctuation)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    stemmed = stem_tokens(filtered, stemmer)\n",
    "    return stemmed\n",
    "\n",
    "processed = np.array(map(preprocess, docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = np.unique(np.hstack(processed.flat))\n",
    "vocab_dict = {}\n",
    "for idx, w in enumerate(vocab):\n",
    "    vocab_dict[w] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Map Docs to Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_as_nums = map(lambda doc: [vocab_dict[w] for w in doc], processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = {}\n",
    "tfidf = {}\n",
    "\n",
    "def freq_map(doc):\n",
    "    out = np.zeros(vocab.size, dtype=np.int32)\n",
    "    for w in doc:\n",
    "        out[w] += 1\n",
    "    return out\n",
    "\n",
    "#def tf(word, doc):\n",
    "#    return \n",
    "    # return 0.5 + 0.5*f[t,d] / max(\n",
    "\n",
    "def n_containing(word, docs):\n",
    "    return sum([1 for doc in docs for w in doc if w == word])\n",
    "\n",
    "def tf(freq, max_freq):\n",
    "    return (0.5 + (0.5*freq/max_freq))\n",
    "\n",
    "def inv_df(word, docs):\n",
    "    if word not in idf:\n",
    "        idf[word]=np.log(len(docs) / (1. + n_containing(word, docs)))\n",
    "    return idf[word]\n",
    "\n",
    "def tfidf(doc):\n",
    "    freq = Counter(doc)\n",
    "    max_freq = np.max(freq.values())\n",
    "    out = np.zeros(vocab.size, dtype=np.float32)\n",
    "    for w in freq.keys():\n",
    "        out[w] = tf(freq[w], max_freq) * inv_df(w, docs_as_nums) \n",
    "    return out\n",
    "            \n",
    "X1 = np.array(map(freq_map, docs_as_nums), dtype=np.int32)\n",
    "X2 = np.array(map(tfidf, docs_as_nums), dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float32') to dtype('int64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-3cfbb3b3d988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lda/lda.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \"\"\"\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lda/lda.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mrands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# FIXME: using numpy.roll with a random shift might be faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lda/lda.pyc\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_to_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lda/utils.pyc\u001b[0m in \u001b[0;36mmatrix_to_lists\u001b[0;34m(doc_word)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mWS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mstartidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(a, repeats, axis)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float32') to dtype('int64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=7, n_iter=500, random_state=1)\n",
    "model.fit(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: friend felt girl time love boyfriend told would girlfriend like didnt one feel want anoth go relationship good person someon made sad mine boy much first realli get got know talk thought day someth long met never thing life start letter said guilti disgust call left found realiz joy best end see month happen make ago knew decid ask asham man think close certain guy also happi discov separ turn around write meet year fact live give could date angri well tri reason went alway quit marri although use shame break refus way help later heard relat togeth care\n",
      "Topic 1: car saw walk man bu night peopl watch alon one way street go film road stop home came drive dark back sit start got tv hous went suddenli accid afraid drunk anoth driver ran see disgust front woman someon tri come dog run could near fear us water hit look fell door room nearli lost outsid two almost young seat light cross get along cut head stand scene travel away behind left began movi follow open turn side full ticket still anim direct old window place swim across scare park countri knock live happen approach pick terribl town ski\n",
      "Topic 2: exam school class pass fail univers examin studi result got year teacher felt student lectur well first get receiv one mark would work good accept time grade happi play go test last heard joy english could expect answer question take select final secondari cours ask find made prepar found day form thought team footbal colleg subject examn match paper game admit enough high won entranc leader finish name knew want told china success classmat medic midterm saw hard standard second chosen primari win bad homework part letter radio make unza lost professor leav afraid enter much done job difficult\n",
      "Topic 3: friend home go went one day come time back came want see place leav got late left parti parent girlfriend boyfriend stay night call holiday promis year hour mother mine meet famili told visit would work get us arriv last birthday talk next school morn even without didnt week ask good two met move drunk long alon lie wait miss new peopl said summer away tell room present seen first togeth felt invit phone live appoint forgot weekend behind look could turn couldnt much close old spent take trip whole sever angri job drink person christma asham roommat hous\n",
      "Topic 4: die year father mother time away day death sister child ill old one sad close night felt brother hospit first grandmoth fear pass heard home friend could rel saw went grandfath ago accid hous sever sleep cri month told face room dog came son long age uncl would live afraid oper fell famili two week suddenli suffer babi know becam bed cousin alon thought later care see doctor children door cancer happen found cat last news younger might realli 2 even dead still dream littl go patient think said young much woke rememb pain gave car eye funer move\n",
      "Topic 5: feel friend person felt peopl disgust thing someth made situat mine angri talk way tri close guilti reason parent mother group asham said treat toward discuss act other realiz even make hurt thought guilt like say famili insult think done caus one sexual someon wrong know understand certain sister without work shame later also father could often anger man rememb look argument classmat use behav public concern mani though realli neg behaviour alway right rel didnt show emot dont fear would quit event help lie opinion speak get problem anoth experi happen bad member told noth physic colleagu see\n",
      "Topic 6: money friend felt mother brother ask found father day someth angri one work someon took caught school book guilti got sister told parent didnt boy without tell time give put back use eat done hous scold take littl refus want accus read buy pay asham broke came stole stolen mine thought tri food neighbour later wrong gave know help thing lost girl certain beat cloth beaten borrow child els hand teacher bad promis howev punish new discov respons get start job disgust could happen lot steal keep shop young two like quarrel classmat student away offic children fight forc\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 100\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 0 0 5 1 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.11322499669705377,\n",
       "  0.12419077817413132,\n",
       "  0.14083762716342979,\n",
       "  0.14532963403355795,\n",
       "  0.1499537587528075,\n",
       "  0.16025895098427798,\n",
       "  0.16620425419474172],\n",
       " [0.14149821640903687,\n",
       "  0.14189456995640112,\n",
       "  0.1426872770511296,\n",
       "  0.14295151274937243,\n",
       "  0.14334786629673668,\n",
       "  0.14374421984410093,\n",
       "  0.14387633769322236])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_map = {0: 'shame', 1: 'anger', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'disgust', 6: 'guilt'}\n",
    "\n",
    "topic_dict = np.zeros(7)\n",
    "emotion_dict = np.zeros(7)\n",
    "p_map = np.zeros((7,7))\n",
    "\n",
    "doc_topic = model.doc_topic_\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    predicted = doc_topic[i].argmax()\n",
    "    actual = labels[i]\n",
    "    topic_dict[predicted] += 1; emotion_dict[actual] += 1\n",
    "    p_map[predicted, actual] += 1\n",
    "\n",
    "print np.argmax(p_map, axis=1)\n",
    "\n",
    "sorted(topic_dict / np.sum(topic_dict)), sorted(emotion_dict / np.sum(emotion_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##supervised lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optparse import OptionParser\n",
    "import sys, re, numpy\n",
    "\n",
    "def load_corpus(filename):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    labelmap = dict()\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        mt = re.match(r'\\[(.+?)\\](.+)', line)\n",
    "        if mt:\n",
    "            label = mt.group(1).split(',')\n",
    "            for x in label: labelmap[x] = 1\n",
    "            line = mt.group(2)\n",
    "        else:\n",
    "            label = None\n",
    "        doc = re.findall(r'\\w+(?:\\'\\w+)?',line.lower())\n",
    "        if len(doc)>0:\n",
    "            corpus.append(doc)\n",
    "            labels.append(label)\n",
    "    f.close()\n",
    "    return labelmap.keys(), corpus, labels\n",
    "\n",
    "class LLDA:\n",
    "    def __init__(self, K, alpha, beta):\n",
    "        #self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def term_to_id(self, term):\n",
    "        if term not in self.vocas_id:\n",
    "            voca_id = len(self.vocas)\n",
    "            self.vocas_id[term] = voca_id\n",
    "            self.vocas.append(term)\n",
    "        else:\n",
    "            voca_id = self.vocas_id[term]\n",
    "        return voca_id\n",
    "\n",
    "    def complement_label(self, label):\n",
    "        if not label: return numpy.ones(len(self.labelmap))\n",
    "        vec = numpy.zeros(len(self.labelmap))\n",
    "        vec[0] = 1.0\n",
    "        for x in label: vec[self.labelmap[x]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def set_corpus(self, labelset, corpus, labels):\n",
    "        labelset.insert(0, \"common\")\n",
    "        self.labelmap = dict(zip(labelset, range(len(labelset))))\n",
    "        self.K = len(self.labelmap)\n",
    "\n",
    "        self.vocas = []\n",
    "        self.vocas_id = dict()\n",
    "        self.labels = numpy.array([self.complement_label(label) for label in labels])\n",
    "        self.docs = [[self.term_to_id(term) for term in doc] for doc in corpus]\n",
    "\n",
    "        M = len(corpus)\n",
    "        V = len(self.vocas)\n",
    "\n",
    "        self.z_m_n = []\n",
    "        self.n_m_z = numpy.zeros((M, self.K), dtype=int)\n",
    "        self.n_z_t = numpy.zeros((self.K, V), dtype=int)\n",
    "        self.n_z = numpy.zeros(self.K, dtype=int)\n",
    "\n",
    "        for m, doc, label in zip(range(M), self.docs, self.labels):\n",
    "            N_m = len(doc)\n",
    "            #z_n = [label[x] for x in numpy.random.randint(len(label), size=N_m)]\n",
    "            z_n = [numpy.random.multinomial(1, label / label.sum()).argmax() for x in range(N_m)]\n",
    "            self.z_m_n.append(z_n)\n",
    "            for t, z in zip(doc, z_n):\n",
    "                self.n_m_z[m, z] += 1\n",
    "                self.n_z_t[z, t] += 1\n",
    "                self.n_z[z] += 1\n",
    "\n",
    "    def inference(self):\n",
    "        V = len(self.vocas)\n",
    "        for m, doc, label in zip(range(len(self.docs)), self.docs, self.labels):\n",
    "            for n in range(len(doc)):\n",
    "                t = doc[n]\n",
    "                z = self.z_m_n[m][n]\n",
    "                self.n_m_z[m, z] -= 1\n",
    "                self.n_z_t[z, t] -= 1\n",
    "                self.n_z[z] -= 1\n",
    "\n",
    "                denom_a = self.n_m_z[m].sum() + self.K * self.alpha\n",
    "                denom_b = self.n_z_t.sum(axis=1) + V * self.beta\n",
    "                p_z = label * (self.n_z_t[:, t] + self.beta) / denom_b * (self.n_m_z[m] + self.alpha) / denom_a\n",
    "                new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "                self.z_m_n[m][n] = new_z\n",
    "                self.n_m_z[m, new_z] += 1\n",
    "                self.n_z_t[new_z, t] += 1\n",
    "                self.n_z[new_z] += 1\n",
    "\n",
    "    def phi(self):\n",
    "        V = len(self.vocas)\n",
    "        return (self.n_z_t + self.beta) / (self.n_z[:, numpy.newaxis] + V * self.beta)\n",
    "\n",
    "    def theta(self):\n",
    "        \"\"\"document-topic distribution\"\"\"\n",
    "        n_alpha = self.n_m_z + self.labels * self.alpha\n",
    "        return n_alpha / n_alpha.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "    def perplexity(self, docs=None):\n",
    "        if docs == None: docs = self.docs\n",
    "        phi = self.phi()\n",
    "        thetas = self.theta()\n",
    "\n",
    "        log_per = N = 0\n",
    "        for doc, theta in zip(docs, thetas):\n",
    "            for w in doc:\n",
    "                log_per -= numpy.log(numpy.inner(phi[:,w], theta))\n",
    "            N += len(doc)\n",
    "        return numpy.exp(log_per / N)\n",
    "\n",
    "def main():\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"-f\", dest=\"filename\", help=\"corpus filename\")\n",
    "    parser.add_option(\"--alpha\", dest=\"alpha\", type=\"float\", help=\"parameter alpha\", default=0.001)\n",
    "    parser.add_option(\"--beta\", dest=\"beta\", type=\"float\", help=\"parameter beta\", default=0.001)\n",
    "    parser.add_option(\"-k\", dest=\"K\", type=\"int\", help=\"number of topics\", default=20)\n",
    "    parser.add_option(\"-i\", dest=\"iteration\", type=\"int\", help=\"iteration count\", default=100)\n",
    "    (options, args) = parser.parse_args()\n",
    "    if not options.filename: parser.error(\"need corpus filename(-f)\")\n",
    "\n",
    "    labelset, corpus, labels = load_corpus(options.filename)\n",
    "\n",
    "    llda = LLDA(options.K, options.alpha, options.beta)\n",
    "    llda.set_corpus(labelset, corpus, labels)\n",
    "\n",
    "    for i in range(options.iteration):\n",
    "        sys.stderr.write(\"-- %d \" % (i + 1))\n",
    "        llda.inference()\n",
    "    #print llda.z_m_n\n",
    "\n",
    "    phi = llda.phi()\n",
    "    for v, voca in enumerate(llda.vocas):\n",
    "        #print ','.join([voca]+[str(x) for x in llda.n_z_t[:,v]])\n",
    "        print ','.join([voca]+[str(x) for x in phi[:,v]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
