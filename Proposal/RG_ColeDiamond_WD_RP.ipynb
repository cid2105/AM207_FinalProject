{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#AM207 | Project Proposal \n",
    "\n",
    "Cole Diamond<br/>\n",
    "Wei Dai<br/>\n",
    "Raphaël Pestourie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is the problem you are addressing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our area of interest is in **Information Retrieval** systems.\n",
    "* The general project idea is to **identify latent topics** in a corpus of documents.\n",
    "* Our corpus will be 10 books from Project Gutenberg \n",
    "* We will use our model to perform inference on a randomly selected page from our corpus to predict which book it originated from.\n",
    "* We will employ **bayes theorem** and **gibbs sampling** to perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What has been done already?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant progress has been made on this problem by researchers in the field of Information Retrieval\n",
    "\n",
    "### TF-IDF\n",
    "#### Description\n",
    "* The text-frequency inverse-document frequency scheme [1] uses a vocabulary of words acros all documents, and, for each document in the corpus, a count is formed of the number of occurrences of each word. \n",
    "* After normalization, the term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a word in the entire corpus \n",
    "#### Advantages <br/>\n",
    "* Tf-idf reduction can perform basic identification of sets of words that are discriminative for documents in the collection\n",
    "##### Shortcomings <br/>\n",
    "* Provides a relatively small amount of reduction in description length \n",
    "* Reveals little in the way of inter or intradocument statistical structure. \n",
    "\n",
    "### LSI\n",
    "\n",
    "* Latent Semantic Indexing [2] uses a singular value decomposition of the X matrix to identify a linear subspace in the space of tf-idf features that captures most of the variance in the collection.\n",
    "* This addresses the reduction problem between does not give information on the inter and intradocument structure \n",
    "\n",
    "### pLSI\n",
    "\n",
    "* The probabilistic Latent Semantic Indexing [3] model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. \n",
    "* pLSI  posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic z.<br/><br/>\n",
    "$$ p(d, w_n)= p(d) \\sum_{z}  p(w_n | z) p(z | d) $$\n",
    "* Each word is generated from a single topic, and different words in a document may be generated from different topics. \n",
    "* Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics\n",
    "#### Shortcomings<br/>\n",
    "(1) The number of parameters in the model grows linearly with the size of the corpus, which leads to overfitting<br/>\n",
    "(2) It is not clear how to assign probability to a document outside of the training set.<br/>\n",
    "\n",
    "\n",
    "### LDA\n",
    "\n",
    "* LDA overcomes both of these problems by treating the topic mixture weights as a k-parameter hidden random variable rather than a large set of individual parameters which are explicitly linked to the training set. \n",
    "* LDA generalizes  easily to new documents. \n",
    "* Furthermore, the parameters in a k-topic LDA model do not grow with the size of the training corpus\n",
    "* Gibbs sampling can be used to perform learning with LDA [5]\n",
    "\n",
    "### Papers\n",
    "<br/>\n",
    "<pre>\n",
    "[1] G. Salton and M. McGill, editors. Introduction to Modern Information Retrieval. McGraw-Hill,\n",
    "1983.\n",
    "\n",
    "[2] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic\n",
    "analysis. Journal of the American Society of Information Science, 41(6):391–407, 1990.\n",
    "\n",
    "[3] T. Hofmann. Probabilistic latent semantic indexing. Proceedings of the Twenty-Second Annual\n",
    "International SIGIR Conference, 1999.\n",
    "\n",
    "[4] Blei, David M and Ng, Andrew Y and Jordan, Michael I, Latent dirichlet allocation.\n",
    "The Journal of machine Learning research, (3):993--1022, 2003\n",
    "\n",
    "[5] Darling, W. M. A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, (Portland, Oregon, USA, 2011).\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What are the questions you are trying to answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18px;\">Given an unseen page from a book, can we predict the book title using its topical composition?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What methodology are you planning to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since LDA seems to be more robust than TF-IDF, pLSI and LSI in topic modeling, we plan to use this technique.\n",
    "* In LDA, documents are represented as random mixtures over latent topics, where each topic is characterized\n",
    "by a distribution over words.\n",
    "* We assume that there are a fixed universe of topics that produce words in a corpus.\n",
    "* The figure below illustrates this model.\n",
    "\n",
    "<img src=\"lda.png\" height=\"200px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes the following generative process for each document\n",
    "$w$ in a corpus $C$:\n",
    "\n",
    "* Choose $N \\sim poisson(\\xi)$.\n",
    "* Choose $\\theta \\sim Dir(\\alpha)$.\n",
    "* For each of the $N$ words $w_n$:\n",
    "    * Choose a topic $z_n \\sim Mult(\\theta)$.\n",
    "    * Choose a word $w_n$ from $p(w_n | z_n,\\beta)$, a multinomial\n",
    "        probability conditioned on the topic $z_n$.\n",
    "        \n",
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic\n",
    "mixture $\\theta$, a set of $N$ topics $z$, and a set of $N$ words $w$ is\n",
    "given by:\n",
    "$$\n",
    "  p(\\theta, z, w, \\alpha, \\beta) = p(\\theta | \\alpha) \\prod_{n=1}^{N} p(z_n | \\theta) p(w_n | z_n,\\beta),\n",
    "$$\n",
    "\n",
    "** 1. We use Gibbs sampling to sample from the conditionals of the posterior of Latent Dirichlet Allocation. **<br/>\n",
    "** 2. We perform inference on a new, unseen document to predict the topics that it references. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What data do you have available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Frankenstein - https://www.gutenberg.org/cache/epub/84/pg84.txt\n",
    "2. The Adventures of Sherlock Holmes - https://www.gutenberg.org/cache/epub/1661/pg1661.txt\n",
    "3. A tale of two cities - https://www.gutenberg.org/cache/epub/98/pg98.txt\n",
    "4. Moby Dick - https://www.gutenberg.org/cache/epub/2701/pg2701.txt\n",
    "5. Beowulf - https://www.gutenberg.org/cache/epub/16328/pg16328.txt\n",
    "6. Dracula - https://www.gutenberg.org/cache/epub/345/pg345.txt\n",
    "7. The Adventures of Huckleberry Finn - https://www.gutenberg.org/cache/epub/76/pg76.txt\n",
    "8. Ulysses - https://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
    "9. The Republic - https://www.gutenberg.org/cache/epub/1497/pg1497.txt\n",
    "10. The Divine Comedy - https://www.gutenberg.org/cache/epub/8800/pg8800.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
