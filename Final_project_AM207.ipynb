{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM207 Final Project: Latent Dirichlet Allocation\n",
    "\n",
    "### Cole Diamond\n",
    "### Wei Dai\n",
    "### Raphael Pestourie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding a list of documents that we want to study (LDA is using a 'bag of words model', there should be an app that transform PDF into bags of words on the internet)\n",
    "2. Preprocess:\n",
    "    a. get a probability of each character and suppress the words with high probability from our dictionary (assign each word to one index)\n",
    "    b. create a random Z array that matches words to a topic number and find the corresponding 'n' 3D matrix\n",
    "3. Sampling with Collapsed Gibbs sampling to get the Z matrix from our data and find the corresponding 'n' 3D matrix\n",
    "4. Recover hidden variables from the Z matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1) Get text data from a list of documents and transform into a bag of words\n",
    "\n",
    "I chose the publications of my former professor Prof. Zhang at UC Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    \"\"\"\n",
    "    This function converts a pdf document from my computer into a str\n",
    "    path: path to the pdf I want to convert\n",
    "    str: utf8 text extracted from the pdf\n",
    "    \"\"\"\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function transform the pdf into strings of text\n",
    "\"\"\"\n",
    "\n",
    "import glob, os\n",
    "import numpy as np\n",
    "\n",
    "# Compute the number of documents in the directory\n",
    "os.chdir(\"/home/raphael/xlab.me.berkeley.edu/pdf\")\n",
    "i=0\n",
    "for file in glob.glob(\"*.pdf\"):\n",
    "    i+=1\n",
    "\n",
    "# create a list of list, each list will be the text file of one pdf\n",
    "documents = [[] for k in range(i)]\n",
    "\n",
    "#fill in the documents list of list\n",
    "for j, file in enumerate(glob.glob(\"*.pdf\")):        \n",
    "    fname=\"/home/raphael/xlab.me.berkeley.edu/pdf/\"+str(file)\n",
    "    documents[j] = convert_pdf_to_txt(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunction to load .txt files and use it as is\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "function to load .txt files and use it as is\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a bag of words (all lower case)\n",
    "import re\n",
    "\n",
    "# clean list\n",
    "documents_wordlist = [[] for k in range(len(documents))]\n",
    "\n",
    "for j, mystr in enumerate(documents):\n",
    "    # take only the words\n",
    "    documents_wordlist[j] = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "    # lowercase them\n",
    "    for k in range(len(documents_wordlist[j])):\n",
    "        documents_wordlist[j][k] = documents_wordlist[j][k].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to save the .txt file\n",
    "\"\"\"\n",
    "f = open(\"documents_wordlist.txt\",\"a\")\n",
    "for doc in documents_wordlist:\n",
    "    for word in doc:\n",
    "        f.write(\"%s \" % word)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2) a) Counting words, create a dictionary and remove the words that are too common or too rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates a set out of the words of 'documents_wordlist'\n",
    "input: documents_wordlist\n",
    "output: set of all the words\n",
    "\"\"\"\n",
    "\n",
    "# create a set with all the words\n",
    "concat = []\n",
    "\n",
    "#concatenate all the word list\n",
    "for i in range(0, len(documents_wordlist)):\n",
    "    concat+= documents_wordlist[i]\n",
    "\n",
    "#create a set\n",
    "set_words = set(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function reduces the set of words we are interested in.\n",
    "It suppress all the words that are to common accross the documents, or too rare\n",
    "\n",
    "input\n",
    "sup: probability above which words are removed\n",
    "inf: probability below which words are removed\n",
    "documents_wordlist: list of document's baggs of words\n",
    "\n",
    "output\n",
    "final_set_words: set of the words we are interested in\n",
    "\"\"\"\n",
    "\n",
    "# create a dictionary for the probabilities of each word \n",
    "# which permits to suppress less common words, or too common ones)\n",
    "\n",
    "#initialization\n",
    "dictionary_process = {word: [] for word in set_words}\n",
    "\n",
    "# create a dictionary where we append the index of the document to the value of the word\n",
    "# we don't append it twice\n",
    "for i in range(len(documents_wordlist)):\n",
    "    for word in documents_wordlist[i]:\n",
    "        if i not in dictionary_process[word]:\n",
    "            dictionary_process[word].append(i)\n",
    "\n",
    "final_set_words = set(set_words)\n",
    "# remove the words that appears in less that 5% of the documents of in \n",
    "# more than 70% (not to take 'Etcheverry' into account)\n",
    "# or more than 80% (to still take 'metal' into account)\n",
    "for word in set_words:\n",
    "    if len(dictionary_process[word])/float(len(documents_wordlist))>.8:\n",
    "        final_set_words.remove(word)\n",
    "    else:\n",
    "        if len(dictionary_process[word])/float(len(documents_wordlist))<.07:\n",
    "            final_set_words.remove(word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911651764442  decrease in the number of words\n"
     ]
    }
   ],
   "source": [
    "print (len(set_words)-len(final_set_words))/float(len(set_words)), \" decrease in the number of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we  wand the number of a word to be always the same\n",
    "global sorted_list\n",
    "sorted_list = sorted(final_set_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_countdict_per_doc(document):\n",
    "    \"\"\"\n",
    "    This function takes a document and the global sorted_list \n",
    "    and creates a dictionnay with all the words in sorted_list with their numbers of occurences in the document\n",
    "    input: document\n",
    "    output: dictionarry\n",
    "    \"\"\"\n",
    "    dictionary_per_doc = {word: 0 for word in sorted_list}\n",
    "    for word in document:\n",
    "        if word in dictionary_per_doc.keys():\n",
    "            dictionary_per_doc[word]+= 1\n",
    "    return dictionary_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict_list(documents_wl):\n",
    "    \"\"\"\n",
    "    This function creates a counting dictionnary for each documents in the document list\n",
    "    This is useful each time we compute the 3D n matrix\n",
    "    input: list of documents\n",
    "    output: list of dictionaries\n",
    "    \"\"\"\n",
    "    dict_list = []\n",
    "    for doc in documents_wl:\n",
    "        dict_list.append(create_countdict_per_doc(doc))\n",
    "    \n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global dict_list\n",
    "dict_list = create_dict_list(documents_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump( dict_list, open( \"save_dict_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2) b) Initialization of Z and function to compute n from Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global D, V\n",
    "D = len(documents_wordlist)\n",
    "V = len(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of topic we want\n",
    "K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = np.random.randint(0,K-1, len(sorted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_3Dmatrix(K, Z):\n",
    "    \"\"\"\n",
    "    This function creates the 3D 'n' matrix that is needed for the collapsed gibbs sampling\n",
    "    one dimension is the topics (this dimension is a list),\n",
    "    the 2 other dimensions is a matrix with card(Z==k) (which varies) columns and D rows\n",
    "    Z: the array that links words to topics\n",
    "    K: number of topic\n",
    "    \"\"\"\n",
    "    \n",
    "    n = []\n",
    "    for k in range(K):\n",
    "        # find the words related to this topic\n",
    "        indexes =  np.where(Z == k)[0]\n",
    "        \n",
    "        # initialize the matrix\n",
    "        count_mat = np.zeros((D, len(indexes)))\n",
    "        \n",
    "        # for each word (that we get thanks to sorted_list) \n",
    "        # go through the documents dictionaries and take the counts\n",
    "        for i, index in enumerate(indexes):\n",
    "            for j in range(D):\n",
    "                count_mat[j, i] = dict_list[j][sorted_list[index]]\n",
    "        \n",
    "        # add the matrix to the list of matrices\n",
    "        n.append(count_mat)\n",
    "    \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3) Sampling with Collapsed Gibbs sampling to get the Z matrix from our data and find the corresponding 'n' 3D matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we need:\n",
    "\n",
    "- dict_list\n",
    "- sorted_list\n",
    "- documents_wordlist (for Gibbs sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(Z_{(m,n)}=k|Z_{-(m,n)},W,\\alpha,\\beta)=\\left(n_{m,(\\cdot)}^{k,-(m,n)}+\\alpha_{k}\\right)\\frac{n_{(\\cdot),v}^{k,-(m,n)}+\\beta_{v}}{\\sum_{r=1}^{V}n_{(\\cdot),r}^{k,-(m,n)}+\\beta_{r}}\n",
    " $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joint_probability_fn(m, n, Z, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute the value of the joint probability using K and Z given the data\n",
    "    Return term 1 * term 2 / term 3\n",
    "    \n",
    "    input\n",
    "    m: doc nb we are on\n",
    "    n: word nb we are on (index in the sorted list) we need an enumerate to find it in the matrix\n",
    "    k: sampled number of topic assigned to (m,n) which count can be zero\n",
    "    Z: \n",
    "    alpha\n",
    "    beta\n",
    "    \n",
    "    output\n",
    "    result: value of the star joint probability\n",
    "    \"\"\"\n",
    "    n_mat = n_3Dmatrix(K, Z)\n",
    "    k = Z[n]\n",
    "    \n",
    "    # indexes of the word in absolute value\n",
    "    indexes =  np.where(Z == k)[0]\n",
    "    \n",
    "    #find index of n in the 'n_mat' 3d matrix\n",
    "    for i, index in enumerate(indexes):\n",
    "        if index==n:\n",
    "            index_searched = i\n",
    "        \n",
    "    # value to substract (this is my understanding of the -(m, n))\n",
    "    substract_val = n_mat[k][m, index_searched]\n",
    "    \n",
    "    # 1st term\n",
    "    term1 = np.sum(n_mat[k][m,:]) - substract_val + alpha[k]\n",
    "    \n",
    "    # 2nd term\n",
    "    # note how tricky it is here, the index in n_mat and the index in beta (that of the sorted_list)\n",
    "    # are not the same !!! Even though of course it refers to the same word.\n",
    "    term2 = np.sum(n_mat[k][:, index_searched]) - substract_val + beta[n]\n",
    "    \n",
    "    # 3rd term\n",
    "    # POTENTIAL ERROR : here I read r, as the word in the columns of the topic matrix\n",
    "    # go back and see what V is\n",
    "    term3 = np.sum([np.sum(n_mat[k][:, i]) - substract_val + beta[index] for i, index in enumerate(indexes)])\n",
    "    \n",
    "    return (term1 * term2 / float(term3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##?\n",
    "\n",
    "alpha and beta are hyperparameter?\n",
    "\n",
    "They never change? (make sure I understood) -> hierarchical Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize alpha\n",
    "alpha = np.ones(K)\n",
    "\n",
    "#initialize beta\n",
    "beta = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iteration_gibbs_WRONG(Z, alpha, beta):\n",
    "    \"\"\"\n",
    "    This function does one iteration of Gibbs sampling\n",
    "    input: \n",
    "    Z: assignment of each word to a topic\n",
    "    \"\"\"\n",
    "    \n",
    "    trace_z1000 = np.zeros(K*D)\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    #train only on 1 tenth of the articles\n",
    "    for m in range(0,D,10):\n",
    "        # ATTENTION! this one might be an issue because the n matrix changes when Z changes\n",
    "        for l in range(K):\n",
    "            trace_z1000[count] = Z[1000] \n",
    "            count+= 1\n",
    "            print Z[1000] \n",
    "            indexes =  np.where(Z == l)[0]\n",
    "            for n in indexes:\n",
    "                Z_star = np.copy(Z)\n",
    "                new_k = np.random.randint(0,K-1)\n",
    "                Z_star[n] = new_k\n",
    "                p_current = joint_probability_fn(m, n, Z, alpha, beta)\n",
    "                p_star = joint_probability_fn(m, n, Z_star, alpha, beta)\n",
    "                ratio = p_star / p_current\n",
    "                if np.random.rand() < ratio:\n",
    "                    Z = np.copy(Z_star)\n",
    "\n",
    "                    \n",
    "    return Z, trace_z1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iteration_gibbs(Z, alpha, beta):\n",
    "    \"\"\"\n",
    "    This function does one iteration of Gibbs sampling\n",
    "    input: \n",
    "    Z: assignment of each word to a topic\n",
    "    \"\"\"\n",
    "    \n",
    "    trace = np.zeros((len(sorted_list), D))\n",
    "    \n",
    "\n",
    "#     go trhough all the word in all the articles\n",
    "    for j, doc in enumerate(documents_wordlist):\n",
    "        trace[:, j] = Z\n",
    "        for word in doc:\n",
    "            for i, word_check in enumerate(sorted_list):\n",
    "#                 check if the word are in the sorted_list\n",
    "                if word_check==word:\n",
    "#                     find the index i\n",
    "                    n=i\n",
    "                    \n",
    "#                     sample a new topic for that word\n",
    "                    Z_star = np.copy(Z)\n",
    "                    new_k = np.random.randint(0,K-1)\n",
    "                    Z_star[n] = new_k\n",
    "#                     calculate joint probabilities\n",
    "                    p_current = joint_probability_fn(j, n, Z, alpha, beta)\n",
    "                    p_star = joint_probability_fn(j, n, Z_star, alpha, beta)\n",
    "                    ratio = p_star / p_current\n",
    "                    if np.random.rand() < ratio:\n",
    "                        Z = np.copy(Z_star)\n",
    "\n",
    "                    \n",
    "    return Z, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Gibbs sampling ####\n",
    "\n",
    "# nb of iteration\n",
    "N_it = 1\n",
    "\n",
    "# tracer for the Z[1000]s, only one value because the whole array is too costly, 1000 is arbitrary\n",
    "tracer_one_value = []\n",
    "\n",
    "#initializaiton of the Z matrix (the only variables we are sampling!!! because it is collapsed Gibbs sampling)\n",
    "Z = np.random.randint(0,K-1, len(sorted_list))\n",
    "\n",
    "for iteration in range(N_it):\n",
    "    Z, tracer_toappend = iteration_gibbs(Z, alpha, beta)\n",
    "    tracer_one_value.append(tracer_toappend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Pray and tmr you'll see it converged\n",
    "plt.plot(tracer_one_value[50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
